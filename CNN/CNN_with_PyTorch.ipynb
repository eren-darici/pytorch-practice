{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqaQ1RVe8lyi"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "l_bIYgDB9reA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 16\n",
        "batch_size = 4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "MexgRuCA9vjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "# Dataset contains PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1].\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "z9IrOm5H95A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBEXYlDT-RgS",
        "outputId": "01e60710-2693-46cd-a45a-a88d1afa6f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "Oo6SeIjp-skx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement ConvNet\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    # feature learning layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "\n",
        "    # classification layers\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "    # flatten\n",
        "    x = x.view(-1, 16*5*5)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "i55SF5rA-39c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "    # input layer = 3 input channels, 6 output channels, 5 kernel size\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Bacward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log\n",
        "    if (i+1) % 2000 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUkUP2_J_EIp",
        "outputId": "1c54205c-a196-4d2d-c8fd-53261000b571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/16], Step [2000/12500], Loss: 2.2863\n",
            "Epoch [1/16], Step [4000/12500], Loss: 2.2358\n",
            "Epoch [1/16], Step [6000/12500], Loss: 2.2148\n",
            "Epoch [1/16], Step [8000/12500], Loss: 2.1886\n",
            "Epoch [1/16], Step [10000/12500], Loss: 2.2241\n",
            "Epoch [1/16], Step [12000/12500], Loss: 1.9391\n",
            "Epoch [2/16], Step [2000/12500], Loss: 2.2906\n",
            "Epoch [2/16], Step [4000/12500], Loss: 1.6531\n",
            "Epoch [2/16], Step [6000/12500], Loss: 2.0232\n",
            "Epoch [2/16], Step [8000/12500], Loss: 2.1837\n",
            "Epoch [2/16], Step [10000/12500], Loss: 2.3546\n",
            "Epoch [2/16], Step [12000/12500], Loss: 1.5428\n",
            "Epoch [3/16], Step [2000/12500], Loss: 1.6040\n",
            "Epoch [3/16], Step [4000/12500], Loss: 1.1669\n",
            "Epoch [3/16], Step [6000/12500], Loss: 1.6386\n",
            "Epoch [3/16], Step [8000/12500], Loss: 1.0692\n",
            "Epoch [3/16], Step [10000/12500], Loss: 1.4504\n",
            "Epoch [3/16], Step [12000/12500], Loss: 1.9131\n",
            "Epoch [4/16], Step [2000/12500], Loss: 1.2350\n",
            "Epoch [4/16], Step [4000/12500], Loss: 0.9926\n",
            "Epoch [4/16], Step [6000/12500], Loss: 1.4357\n",
            "Epoch [4/16], Step [8000/12500], Loss: 1.0256\n",
            "Epoch [4/16], Step [10000/12500], Loss: 2.4092\n",
            "Epoch [4/16], Step [12000/12500], Loss: 1.0460\n",
            "Epoch [5/16], Step [2000/12500], Loss: 1.7941\n",
            "Epoch [5/16], Step [4000/12500], Loss: 1.0346\n",
            "Epoch [5/16], Step [6000/12500], Loss: 1.7532\n",
            "Epoch [5/16], Step [8000/12500], Loss: 1.3937\n",
            "Epoch [5/16], Step [10000/12500], Loss: 1.6032\n",
            "Epoch [5/16], Step [12000/12500], Loss: 1.2834\n",
            "Epoch [6/16], Step [2000/12500], Loss: 1.2378\n",
            "Epoch [6/16], Step [4000/12500], Loss: 0.9246\n",
            "Epoch [6/16], Step [6000/12500], Loss: 0.4844\n",
            "Epoch [6/16], Step [8000/12500], Loss: 1.4706\n",
            "Epoch [6/16], Step [10000/12500], Loss: 2.2066\n",
            "Epoch [6/16], Step [12000/12500], Loss: 1.3414\n",
            "Epoch [7/16], Step [2000/12500], Loss: 1.7268\n",
            "Epoch [7/16], Step [4000/12500], Loss: 1.1109\n",
            "Epoch [7/16], Step [6000/12500], Loss: 2.2333\n",
            "Epoch [7/16], Step [8000/12500], Loss: 1.6810\n",
            "Epoch [7/16], Step [10000/12500], Loss: 0.5140\n",
            "Epoch [7/16], Step [12000/12500], Loss: 1.1238\n",
            "Epoch [8/16], Step [2000/12500], Loss: 2.5996\n",
            "Epoch [8/16], Step [4000/12500], Loss: 0.8892\n",
            "Epoch [8/16], Step [6000/12500], Loss: 0.7712\n",
            "Epoch [8/16], Step [8000/12500], Loss: 0.2988\n",
            "Epoch [8/16], Step [10000/12500], Loss: 1.0516\n",
            "Epoch [8/16], Step [12000/12500], Loss: 2.0509\n",
            "Epoch [9/16], Step [2000/12500], Loss: 0.8997\n",
            "Epoch [9/16], Step [4000/12500], Loss: 0.5863\n",
            "Epoch [9/16], Step [6000/12500], Loss: 1.6808\n",
            "Epoch [9/16], Step [8000/12500], Loss: 1.0244\n",
            "Epoch [9/16], Step [10000/12500], Loss: 1.3120\n",
            "Epoch [9/16], Step [12000/12500], Loss: 2.2208\n",
            "Epoch [10/16], Step [2000/12500], Loss: 0.5112\n",
            "Epoch [10/16], Step [4000/12500], Loss: 1.6110\n",
            "Epoch [10/16], Step [6000/12500], Loss: 1.9106\n",
            "Epoch [10/16], Step [8000/12500], Loss: 1.7304\n",
            "Epoch [10/16], Step [10000/12500], Loss: 2.1613\n",
            "Epoch [10/16], Step [12000/12500], Loss: 0.9446\n",
            "Epoch [11/16], Step [2000/12500], Loss: 0.7818\n",
            "Epoch [11/16], Step [4000/12500], Loss: 0.5156\n",
            "Epoch [11/16], Step [6000/12500], Loss: 0.6702\n",
            "Epoch [11/16], Step [8000/12500], Loss: 0.7483\n",
            "Epoch [11/16], Step [10000/12500], Loss: 0.8902\n",
            "Epoch [11/16], Step [12000/12500], Loss: 1.1091\n",
            "Epoch [12/16], Step [2000/12500], Loss: 1.0280\n",
            "Epoch [12/16], Step [4000/12500], Loss: 1.3198\n",
            "Epoch [12/16], Step [6000/12500], Loss: 0.5729\n",
            "Epoch [12/16], Step [8000/12500], Loss: 1.8537\n",
            "Epoch [12/16], Step [10000/12500], Loss: 1.3660\n",
            "Epoch [12/16], Step [12000/12500], Loss: 0.6438\n",
            "Epoch [13/16], Step [2000/12500], Loss: 1.3938\n",
            "Epoch [13/16], Step [4000/12500], Loss: 1.3366\n",
            "Epoch [13/16], Step [6000/12500], Loss: 0.6728\n",
            "Epoch [13/16], Step [8000/12500], Loss: 1.2888\n",
            "Epoch [13/16], Step [10000/12500], Loss: 1.0412\n",
            "Epoch [13/16], Step [12000/12500], Loss: 0.3137\n",
            "Epoch [14/16], Step [2000/12500], Loss: 1.0645\n",
            "Epoch [14/16], Step [4000/12500], Loss: 1.3644\n",
            "Epoch [14/16], Step [6000/12500], Loss: 0.9340\n",
            "Epoch [14/16], Step [8000/12500], Loss: 1.0370\n",
            "Epoch [14/16], Step [10000/12500], Loss: 1.3674\n",
            "Epoch [14/16], Step [12000/12500], Loss: 1.5751\n",
            "Epoch [15/16], Step [2000/12500], Loss: 1.2543\n",
            "Epoch [15/16], Step [4000/12500], Loss: 2.2613\n",
            "Epoch [15/16], Step [6000/12500], Loss: 1.8843\n",
            "Epoch [15/16], Step [8000/12500], Loss: 1.2723\n",
            "Epoch [15/16], Step [10000/12500], Loss: 1.2968\n",
            "Epoch [15/16], Step [12000/12500], Loss: 0.6069\n",
            "Epoch [16/16], Step [2000/12500], Loss: 0.5978\n",
            "Epoch [16/16], Step [4000/12500], Loss: 0.4643\n",
            "Epoch [16/16], Step [6000/12500], Loss: 0.7386\n",
            "Epoch [16/16], Step [8000/12500], Loss: 2.0011\n",
            "Epoch [16/16], Step [10000/12500], Loss: 0.4186\n",
            "Epoch [16/16], Step [12000/12500], Loss: 0.4841\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "  for images, labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "\n",
        "    # max returns (value, index)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0)\n",
        "    n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = predicted[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Accuracy of the network: {acc}%')\n",
        "\n",
        "  for i in range(10):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSg8TJW1Efxg",
        "outputId": "3f876c1b-1f14-431e-a16f-09fb23fd1376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 63.11%\n",
            "Accuracy of plane: 73.4%\n",
            "Accuracy of car: 76.6%\n",
            "Accuracy of bird: 49.7%\n",
            "Accuracy of cat: 32.4%\n",
            "Accuracy of deer: 54.8%\n",
            "Accuracy of dog: 51.5%\n",
            "Accuracy of frog: 82.8%\n",
            "Accuracy of horse: 57.8%\n",
            "Accuracy of ship: 84.2%\n",
            "Accuracy of truck: 67.9%\n"
          ]
        }
      ]
    }
  ]
}